# -*- coding: utf-8 -*-
"""
DG å®ç›˜æ£€æµ‹å™¨ â€” GitHub Actions ç‰ˆï¼ˆæ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ï¼‰
åŠŸèƒ½ï¼š
 - ä½¿ç”¨ Playwright è‡ªåŠ¨è¿›å…¥ DG é¡µé¢ (å°è¯•ä¸¤ä¸ªå…¥å£)
 - æ¨¡æ‹Ÿç‚¹å‡» Free / å…è´¹è¯•ç©ã€æ¨¡æ‹Ÿæ»šåŠ¨/æ‹–åŠ¨æ»‘å—ï¼ˆå¤šæ¬¡å°è¯•ï¼‰
 - æˆªå›¾å¹¶ä½¿ç”¨ OpenCV/NumPy/Scikit-learn åˆ†ææ¯æ¡Œç å­åˆ†å¸ƒ
 - ä¸¥æ ¼æŒ‰ç”¨æˆ·è§„åˆ™åˆ¤æ–­ï¼š
    * é•¿è¿ >=4 (longish)
    * é¾™ = è¿ç»­ >=8 (long)
    * è¶…é¾™ = è¿ç»­ >=10 (super_long)
    * è¿ç /å¤šè¿ï¼šåŒä¸€è¡Œå‡ºç°è¿ç»­ >=4 çš„æ¨ªå‘è¿ (horizontal run >=4)
    * è¿ç»­3æ’å¤šè¿ï¼šæ£€æµ‹åˆ°ä»»æ„ 3 ä¸ª**è¿ç»­è¡Œ**ï¼ˆrow r,r+1,r+2ï¼‰æ¯è¡Œå‡æœ‰æ¨ªå‘è¿ >=4
 - åˆ¤å®šæ€»ä½“ï¼š
    * æ”¾æ°´æ—¶æ®µï¼šæ»¡è¶³ è¶…é•¿é¾™+2é•¿é¾™ï¼ˆè¶…é¾™>=1 and é•¿é¾™>=2 and æ€» >=3ï¼‰ OR é•¿é¾™/è¶…é¾™çš„æ¡Œæ•° >= MIN_BOARDS_FOR_PAW
    * ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰ï¼šè‡³å°‘ 3 å¼ æ¡Œå­æœ‰â€œè¿ç»­3æ’å¤šè¿â€ ä¸” è‡³å°‘ 2 å¼ æ¡Œå­ä¸ºé•¿é¾™/è¶…é•¿é¾™ï¼ˆå¯ä»¥ä¸ºåŒä¸€æ¡Œï¼‰
    * å…¶ä½™åˆ¤å®šä¸ºèƒœç‡ä¸­ç­‰æˆ–èƒœç‡è°ƒä½ ï¼ˆæŒ‰ç¨€ç–åº¦åˆ¤æ–­ï¼‰
 - å½“è¿›å…¥ æ”¾æ°´ æˆ– ä¸­ç­‰ï¼ˆä¸­ä¸Šï¼‰æ—¶å‘é€ Telegram å¼€å§‹é€šçŸ¥ï¼ˆå«ä¼°ç®—ç»“æŸæ—¶é—´åŸºäºå†å²å¹³å‡ï¼‰ï¼Œå¹¶è¿›å…¥æ´»åŠ¨çŠ¶æ€ï¼ˆä¸ä¼šé‡å¤æé†’ï¼‰
 - å½“æ´»åŠ¨ç»“æŸæ—¶å‘é€ Telegram ç»“æŸé€šçŸ¥ï¼ˆå«çœŸå®æŒç»­åˆ†é’Ÿæ•°ï¼‰ï¼Œå¹¶ä¿å­˜å†å²
 - è¾“å‡º last_run_summary.json ä¾›è°ƒè¯•
"""

import os, sys, time, json, math, random
from datetime import datetime, timedelta, timezone
import requests
import numpy as np
from io import BytesIO
from pathlib import Path

# image libs
import cv2
from PIL import Image

# clustering
from sklearn.cluster import KMeans

# playwright
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout

# config / env
TG_TOKEN = os.environ.get("TG_BOT_TOKEN", "").strip()
TG_CHAT  = os.environ.get("TG_CHAT_ID", "").strip()
DG_LINKS = [
    "https://dg18.co/wap/",
    "https://dg18.co/"
]
MIN_BOARDS_FOR_PAW = int(os.environ.get("MIN_BOARDS_FOR_PAW","3"))
MID_LONG_REQ = int(os.environ.get("MID_LONG_REQ","2"))
COOLDOWN_MINUTES = int(os.environ.get("COOLDOWN_MINUTES","10"))

STATE_FILE = "state.json"
SUMMARY_FILE = "last_run_summary.json"
TZ = timezone(timedelta(hours=8))

def log(msg):
    now = datetime.now(TZ).strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{now}] {msg}", flush=True)

# Telegram helper
def send_telegram(text):
    if not TG_TOKEN or not TG_CHAT:
        log("Telegram æœªé…ç½®ï¼ˆTG_BOT_TOKEN æˆ– TG_CHAT_ID ä¸ºç©ºï¼‰ï¼Œè·³è¿‡å‘é€ã€‚")
        return False
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    payload = {"chat_id":TG_CHAT, "text": text, "parse_mode":"HTML"}
    try:
        r = requests.post(url, data=payload, timeout=20)
        j = r.json()
        if j.get("ok"):
            log("Telegram å‘é€æˆåŠŸã€‚")
            return True
        else:
            log(f"Telegram API è¿”å›: {j}")
            return False
    except Exception as e:
        log(f"å‘é€ Telegram å¤±è´¥: {e}")
        return False

# state
def load_state():
    if not os.path.exists(STATE_FILE):
        s = {"active":False,"kind":None,"start_time":None,"last_seen":None,"history":[]}
        return s
    try:
        with open(STATE_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        log(f"è¯»å– state.json å‡ºé”™: {e}")
        return {"active":False,"kind":None,"start_time":None,"last_seen":None,"history":[]}

def save_state(s):
    with open(STATE_FILE,"w",encoding="utf-8") as f:
        json.dump(s, f, ensure_ascii=False, indent=2)

# image utilities
def pil_from_bytes(bts):
    return Image.open(BytesIO(bts)).convert("RGB")

def cv_from_pil(pil):
    return cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)

# detect red and blue bead centers robustly
def detect_beads(img_bgr):
    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
    # red thresholds (two ranges)
    lower1 = np.array([0,100,70]); upper1 = np.array([8,255,255])
    lower2 = np.array([160,80,70]); upper2 = np.array([179,255,255])
    mask_r = cv2.inRange(hsv, lower1, upper1) | cv2.inRange(hsv, lower2, upper2)
    # blue
    lowerb = np.array([90,60,50]); upperb = np.array([140,255,255])
    mask_b = cv2.inRange(hsv, lowerb, upperb)

    # clean
    k = np.ones((3,3), np.uint8)
    mask_r = cv2.morphologyEx(mask_r, cv2.MORPH_OPEN, k, iterations=1)
    mask_b = cv2.morphologyEx(mask_b, cv2.MORPH_OPEN, k, iterations=1)

    points = []
    # find centers using contours
    for mask, label in [(mask_r,'B'), (mask_b,'P')]:
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area < 8: continue
            M = cv2.moments(cnt)
            if M["m00"] == 0: continue
            cx = int(M["m10"]/M["m00"]); cy = int(M["m01"]/M["m00"])
            points.append((cx,cy,label))
    return points, mask_r, mask_b

# cluster points into board regions (grid-based + merge)
def cluster_boards(points, w, h):
    if not points:
        return []
    # coarse cell size derived from image size
    cell = max(60, int(min(w,h)/12))
    cols = math.ceil(w/cell); rows = math.ceil(h/cell)
    grid = [[0]*cols for _ in range(rows)]
    for (x,y,_) in points:
        cx = min(cols-1, x//cell); cy = min(rows-1, y//cell)
        grid[cy][cx] += 1
    hits=[]
    thr = max(3, int(len(points)/(6*max(1,min(cols,rows)))))  # adaptive threshold
    for r in range(rows):
        for c in range(cols):
            if grid[r][c] >= thr:
                hits.append((r,c))
    if not hits:
        # fallback KMeans to cluster into up to 8 regions
        coords = np.array([[p[0], p[1]] for p in points], dtype=float)
        k = min(8, max(1, len(points)//15))
        kmeans = KMeans(n_clusters=k, random_state=0).fit(coords)
        regions=[]
        for lab in range(k):
            pts = coords[kmeans.labels_==lab]
            if len(pts)==0: continue
            x0,y0 = pts.min(axis=0); x1,y1 = pts.max(axis=0)
            regions.append((int(max(0,x0-8)), int(max(0,y0-8)), int(min(w, x1-x0+16)), int(min(h, y1-y0+16))))
        return regions
    rects=[]
    for (r,c) in hits:
        x = c*cell; y = r*cell; wcell = cell; hcell = cell
        placed=False
        for i,(rx,ry,rw,rh) in enumerate(rects):
            if not (x > rx+rw+cell or x+wcell < rx-cell or y > ry+rh+cell or y+hcell < ry-cell):
                nx = min(rx,x); ny = min(ry,y)
                nw = max(rx+rw, x+wcell) - nx
                nh = max(ry+rh, y+hcell) - ny
                rects[i] = (nx,ny,nw,nh)
                placed=True; break
        if not placed:
            rects.append((x,y,wcell,hcell))
    regions=[]
    for (x,y,w0,h0) in rects:
        nx=max(0,x-10); ny=max(0,y-10)
        nw=min(w-nx, w0+20); nh=min(h-ny, h0+20)
        regions.append((int(nx),int(ny),int(nw),int(nh)))
    return regions

# analyze single board region: build matrix of rows x cols, compute runs and horizontal runs
def analyze_region(img_bgr, region):
    x,y,w,h = region
    crop = img_bgr[y:y+h, x:x+w]
    pts, _, _ = detect_beads(crop)
    if not pts:
        return {"total":0,"maxRun":0,"category":"empty","has_multirow":False,"runs":[],"grid":None}
    # positions
    coords = np.array([[p[0], p[1]] for p in pts])
    labels = [p[2] for p in pts]
    # estimate number of columns: try kmeans on x with k up to 12
    est_cols = min(18, max(3, int(w / max(20, w//12))))
    # try multiple k to find stable clustering using inertia heuristic
    best_k = min(est_cols, max(3, len(coords)//6))
    if len(coords) < 8:
        best_k = max(1, len(coords)//2)
    # if few points, fallback to simple column grouping by binning
    if len(coords) < 6:
        # bin by x into ~5 bins
        bins = max(1, min(6, int(w/60)))
        xs = coords[:,0]
        cols_idx = np.floor(xs / (w / max(1,bins))).astype(int)
        unique_cols = sorted(set(cols_idx))
        col_positions = []
        for uc in unique_cols:
            idx = np.where(cols_idx==uc)[0]
            col_positions.append([coords[i][0] for i in idx])
        # build sequences per column
        sequences=[]
        for uc in unique_cols:
            idx = np.where(cols_idx==uc)[0]
            col_pts = sorted([(coords[i][1], labels[i]) for i in idx], key=lambda t:t[0])
            sequences.append([lab for (_,lab) in col_pts])
    else:
        # use kmeans on x to group into columns
        X = coords[:,0].reshape(-1,1)
        K = min(best_k, max(2, len(coords)//3))
        try:
            kmeans = KMeans(n_clusters=K, random_state=0).fit(X)
            groups = [[] for _ in range(K)]
            for i,lab in enumerate(kmeans.labels_):
                groups[lab].append(i)
            # order columns by centroid x
            centroids = kmeans.cluster_centers_.flatten()
            order = sorted(range(K), key=lambda i: centroids[i])
            sequences=[]
            for oi in order:
                idxs = groups[oi]
                col_pts = sorted([(coords[i][1], labels[i]) for i in idxs], key=lambda t:t[0])
                sequences.append([lab for (_,lab) in col_pts])
        except Exception:
            # fallback to binning
            bins = max(1, min(6, int(w/60)))
            xs = coords[:,0]
            cols_idx = np.floor(xs / (w / max(1,bins))).astype(int)
            unique_cols = sorted(set(cols_idx))
            sequences=[]
            for uc in unique_cols:
                idx = np.where(cols_idx==uc)[0]
                col_pts = sorted([(coords[i][1], labels[i]) for i in idx], key=lambda t:t[0])
                sequences.append([lab for (_,lab) in col_pts])

    # flatten into bead reading order (column-major top->bottom, left->right)
    maxlen = max((len(s) for s in sequences), default=0)
    flattened=[]
    for r in range(maxlen):
        for col in sequences:
            if r < len(col):
                flattened.append(col[r])
    # compute vertical/flatten runs
    runs=[]
    if flattened:
        cur={"color":flattened[0],"len":1}
        for i in range(1,len(flattened)):
            if flattened[i]==cur["color"]:
                cur["len"]+=1
            else:
                runs.append(cur); cur={"color":flattened[i],"len":1}
        runs.append(cur)
    maxRun = max((r["len"] for r in runs), default=0)
    # build approximate grid by assigning row indices from sorted unique y's per column
    # For horizontal run detection, we approximate rows by quantizing y positions across all pts
    ys = sorted(set([int(round(p[1])) for p in coords[:,1]]))
    if len(ys) == 0:
        grid = None
    else:
        # cluster y into rows using kmeans on y
        try:
            rows_k = min(len(ys), max(3, int(h/28)))
            y_coords = np.array(ys).reshape(-1,1)
            if len(y_coords) >= rows_k:
                ky = KMeans(n_clusters=rows_k, random_state=0).fit(y_coords)
                centers = sorted([c[0] for c in ky.cluster_centers_])
                # map each point to nearest center index
                row_indices = [int(np.argmin([abs(p[1]-c) for c in centers])) for p in pts]
                col_count = len(sequences)
                row_count = len(centers)
                grid = [['' for _ in range(col_count)] for __ in range(row_count)]
                # place points by nearest col index (use sequences order centroids)
                # approximate column_x positions from sequences by mean x per col
                col_xs = []
                # recover mean x of each sequence by scanning original coords mapping
                # compute col_x for each sequence by averaging xs of corresponding points
                # Here we reconstruct groups by counting sequence lengths may not give indices; fallback reasonable
                for seq in sequences:
                    # find average x among the labels corresponding - rough estimate
                    col_xs.append(np.mean([coords[i][0] for i in range(len(coords))]) if len(coords)>0 else 0)
                # instead use KMeans centroids earlier if available (we didn't save), so fallback to quantize by x bins
                xs_all = coords[:,0]
                col_bins = np.linspace(0, crop.shape[1], num=max(2,len(sequences)+1))
                for idx_pt, (px,py,lab) in enumerate(pts):
                    col_idx = np.searchsorted(col_bins, px) - 1
                    col_idx = max(0, min(len(sequences)-1, col_idx))
                    row_idx = row_indices[idx_pt]
                    grid[row_idx][col_idx] = lab
        except Exception:
            grid = None

    # check horizontal runs per row (if grid available)
    has_multirow = False
    if grid:
        # compute for each row longest horizontal same-color run
        row_runs = []
        for r in range(len(grid)):
            maxh = 0
            curc = None; curlen=0
            for c in range(len(grid[0])):
                v = grid[r][c]
                if v == curc and v != '':
                    curlen += 1
                else:
                    curc = v
                    curlen = 1 if v != '' else 0
                if curlen > maxh: maxh = curlen
            row_runs.append(maxh)
        # find any 3 consecutive rows each with horizontal run >=4
        for i in range(0, max(0,len(row_runs)-2)):
            if row_runs[i] >=4 and row_runs[i+1] >=4 and row_runs[i+2] >=4:
                has_multirow = True
                break

    # classify
    cat = "other"
    if maxRun >= 10: cat = "super_long"
    elif maxRun >= 8: cat = "long"
    elif maxRun >= 4: cat = "longish"
    elif maxRun == 1: cat = "single"

    return {"total":len(flattened),"maxRun":maxRun,"category":cat,"has_multirow":has_multirow,"runs":runs,"grid":grid}

# take screenshot via Playwright with robust attempts
def capture_screenshot(play, url, tries=2):
    log(f"å°è¯•æ‰“å¼€ {url}")
    browser = None
    try:
        browser = play.chromium.launch(headless=True, args=[
            "--no-sandbox","--disable-setuid-sandbox",
            "--disable-dev-shm-usage","--disable-accelerated-2d-canvas"
        ])
        context = browser.new_context(viewport={"width":1280,"height":900}, user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/115.0 Safari/537.36")
        page = context.new_page()
        page.set_default_timeout(35000)
        page.goto(url)
        time.sleep(2+random.random()*1.5)
        # try click common Free buttons (multi-language)
        texts = ["Free","å…è´¹è¯•ç©","å…è´¹","Play Free","è¯•ç©","è¿›å…¥","Free Play"]
        clicked=False
        for t in texts:
            try:
                el = page.locator(f"text={t}")
                if el.count()>0:
                    try:
                        el.first.click(timeout=3000)
                        clicked=True
                        log(f"ç‚¹å‡»æŒ‰é’®: {t}")
                        break
                    except Exception:
                        pass
            except Exception:
                pass
        # try to detect and handle slider or scroll security
        try:
            # scroll whole page to trigger lazy elements
            page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(0.8)
            page.evaluate("window.scrollTo(0, 0);")
            time.sleep(0.8)
            # attempt some mouse wheel actions
            for _ in range(3):
                page.mouse.wheel(0, 400)
                time.sleep(0.4)
        except Exception:
            pass
        # wait a few seconds for DG content to load
        time.sleep(3 + random.random()*1.5)
        # if there is an iframe with the table, try to screenshot entire viewport
        try:
            shot = page.screenshot(full_page=True)
            log("å·²æˆªå– full_page æˆªå›¾ã€‚")
            context.close()
            return shot
        except Exception:
            try:
                # fallback viewport screenshot
                shot = page.screenshot()
                context.close()
                return shot
            except Exception as e:
                log(f"æˆªå›¾å¤±è´¥: {e}")
                context.close()
                return None
    except Exception as e:
        log(f"Playwright è®¿é—®å‡ºé”™: {e}")
        if browser:
            try: browser.close()
            except: pass
        return None

# classify overall using the strict rules you demanded
def classify_overall(board_stats):
    long_count = sum(1 for b in board_stats if b['category'] in ('long','super_long'))
    super_count = sum(1 for b in board_stats if b['category']=='super_long')
    multirow_count = sum(1 for b in board_stats if b.get('has_multirow',False))
    # æ”¾æ°´ï¼šè¶…é•¿é¾™è§¦å‘å‹ OR æ»¡ç›˜é•¿è¿å‹ (è¿™é‡Œå®ç°ä¸¤å¥—)
    # è¶…é•¿é¾™è§¦å‘å‹: è‡³å°‘ 1 è¶…é¾™ && è‡³å°‘ 2 é•¿é¾™ && (super+long) >=3
    if super_count >= 1 and long_count >= 2 and (super_count + long_count) >= 3:
        return "æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", long_count, super_count, multirow_count
    # æ»¡ç›˜é•¿è¿: è‹¥æ»¡è¶³ MIN_BOARDS_FOR_PAW å¼ æ¡Œæ˜¯ é•¿é¾™/è¶…é•¿é¾™
    if (long_count + super_count) >= MIN_BOARDS_FOR_PAW:
        return "æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", long_count, super_count, multirow_count
    # ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰: è‡³å°‘ 3 å¼ æ¡Œå­æ»¡è¶³ è¿ç»­3æ’å¤šè¿ && è‡³å°‘ 2 å¼ ä¸º é•¿é¾™/è¶…é•¿é¾™ (å¯åŒæ¡Œ)
    if multirow_count >= 3 and (long_count + super_count) >= 2:
        return "ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰", long_count, super_count, multirow_count
    # è‹¥å¤šæ•°æ¡Œå¾ˆç¨€ç–åˆ™ä¸º æ”¶å‰²
    totals = [b['total'] for b in board_stats]
    sparse_count = sum(1 for t in totals if t < 6)
    if board_stats and sparse_count >= len(board_stats)*0.6:
        return "èƒœç‡è°ƒä½ / æ”¶å‰²æ—¶æ®µ", long_count, super_count, multirow_count
    return "èƒœç‡ä¸­ç­‰ï¼ˆå¹³å°æ”¶å‰²ä¸­ç­‰æ—¶æ®µï¼‰", long_count, super_count, multirow_count

# main
def main():
    state = load_state()
    log("=== æ–°ä¸€æ¬¡æ£€æµ‹å¼€å§‹ ===")
    screenshot = None
    with sync_playwright() as p:
        for url in DG_LINKS:
            try:
                screenshot = capture_screenshot(p, url)
                if screenshot:
                    break
            except Exception as e:
                log(f"è®¿é—® {url} å‡ºé”™: {e}")
                continue
    if not screenshot:
        log("æœªèƒ½è·å¾—é¡µé¢æˆªå›¾ï¼Œæœ¬æ¬¡ run ç»“æŸã€‚")
        save_state(state)
        return

    pil = pil_from_bytes(screenshot)
    bgr = cv_from_pil(pil)
    h, w = bgr.shape[:2]
    points, mr, mb = detect_beads(bgr)
    log(f"æ£€æµ‹åˆ°ç‚¹æ•°: {len(points)}")
    if len(points) < 8:
        log("ç‚¹å¤ªå°‘ï¼Œå¯èƒ½é¡µé¢æœªå®Œå…¨åŠ è½½æˆ–é€‰æ‹©é”™è¯¯ï¼ˆå¯èƒ½ä¸æ˜¯å±€åŠ¿é¡µé¢ï¼‰")
    regions = cluster_boards(points, w, h)
    log(f"èšç±»å‡º {len(regions)} ä¸ªå€™é€‰æ¡ŒåŒº")
    board_stats = []
    for r in regions:
        st = analyze_region(bgr, r)
        board_stats.append(st)
    # if no regions, fallback: attempt to divide whole page into grid and analyze each
    if not board_stats:
        # consider 6x4 grid
        gcols = 4; grows = 6
        wstep = w//gcols; hstep = h//grows
        for gy in range(grows):
            for gx in range(gcols):
                rx = gx*wstep; ry = gy*hstep; rw = wstep; rh = hstep
                st = analyze_region(bgr, (rx,ry,rw,rh))
                if st['total']>0:
                    board_stats.append(st)

    overall, long_count, super_count, multirow_count = classify_overall(board_stats)
    log(f"æœ¬æ¬¡åˆ¤å®šï¼š{overall} (é•¿é¾™æ•°={long_count} è¶…é•¿é¾™={super_count} è¿ç»­3æ’å¤šè¿æ¡Œæ•°={multirow_count} )")

    now = datetime.now(TZ)
    now_iso = now.isoformat()
    was_active = state.get("active", False)
    is_active = overall in ("æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", "ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰")

    if is_active and not was_active:
        # å¼€å§‹æ–°äº‹ä»¶
        history = state.get("history", [])
        est_minutes = None
        durations = [h.get("duration_minutes",0) for h in history if h.get("duration_minutes",0)>0]
        if durations:
            est_minutes = max(1, round(sum(durations)/len(durations)))
        else:
            est_minutes = 10  # fallback
        est_end = (now + timedelta(minutes=est_minutes)).astimezone(TZ).strftime("%Y-%m-%d %H:%M:%S")
        emoji = "ğŸŸ¢" if overall.startswith("æ”¾æ°´") else "ğŸ”µ"
        msg = f"{emoji} <b>DG å±€åŠ¿æé†’ â€” {overall}</b>\nå¼€å§‹: {now_iso}\né•¿é¾™æ•°: {long_count}ï¼›è¶…é•¿é¾™: {super_count}ï¼›è¿ç»­3æ’å¤šè¿æ¡Œ: {multirow_count}\nä¼°è®¡ç»“æŸ: {est_end}ï¼ˆçº¦ {est_minutes} åˆ†é’Ÿï¼ŒåŸºäºå†å²ï¼‰\n\nå¦‚è¦æ‰‹åŠ¨å…¥åœºï¼Œè¯·æ³¨æ„é£é™©ã€‚"
        send_telegram(msg)
        # update state
        state = {"active":True, "kind":overall, "start_time":now_iso, "last_seen":now_iso, "history": state.get("history",[])}
        save_state(state)
        log("å·²è®°å½•æ´»åŠ¨å¼€å§‹å¹¶å‘é€é€šçŸ¥ã€‚")

    elif is_active and was_active:
        # æŒç»­ä¸­çš„æ´»åŠ¨ -> æ›´æ–° last_seen
        state["last_seen"] = now_iso
        state["kind"] = overall
        save_state(state)
        log("ä»åœ¨æ´»åŠ¨ä¸­ï¼Œæ›´æ–° last_seenã€‚")

    elif (not is_active) and was_active:
        # æ´»åŠ¨ç»“æŸ
        start = datetime.fromisoformat(state.get("start_time"))
        end = now
        duration_minutes = round((end - start).total_seconds() / 60.0)
        history = state.get("history", [])
        history.append({"kind": state.get("kind"), "start_time": state.get("start_time"), "end_time": end.isoformat(), "duration_minutes": duration_minutes})
        # cap history length
        history = history[-120:]
        # save
        new_state = {"active":False, "kind":None, "start_time":None, "last_seen":None, "history": history}
        save_state(new_state)
        msg = f"ğŸ”´ <b>DG æ”¾æ°´/ä¸­ä¸Š å·²ç»“æŸ</b>\nç±»å‹: {state.get('kind')}\nå¼€å§‹: {state.get('start_time')}\nç»“æŸ: {end.isoformat()}\nå®é™…æŒç»­: {duration_minutes} åˆ†é’Ÿ"
        send_telegram(msg)
        log("äº‹ä»¶ç»“æŸé€šçŸ¥å·²å‘é€å¹¶è®°å½•å†å²ã€‚")
    else:
        # not active, do nothing
        save_state(state)
        log("å½“å‰ä¸åœ¨æ”¾æ°´/ä¸­ä¸Šæ—¶æ®µï¼Œä¸å‘é€æé†’ã€‚")

    # save summary file for debugging
    summary = {"ts": now_iso, "overall":overall, "long_count":long_count, "super_count":super_count, "multirow_count":multirow_count, "boards": board_stats[:40]}
    with open(SUMMARY_FILE,"w",encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"æœªæ•è·å¼‚å¸¸: {e}")
        raise
