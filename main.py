# -*- coding: utf-8 -*-
"""
DG ç›‘æµ‹è„šæœ¬ï¼ˆæœ€ç»ˆç‰ˆï¼‰
- ä¼˜å…ˆè§†è§‰è¯†åˆ«ï¼ˆPlaywright æˆªå›¾ + OpenCVï¼‰
- è‹¥æ— æ³•è¿›å…¥å®ç›˜æˆ–æˆªå›¾ç‚¹æ•°ä¸è¶³ -> è‡ªåŠ¨é€€å›ç½‘ç»œæ¨¡å¼ (æ•è·é¡µé¢æ‰€æœ‰ XHR/Fetch JSON) å¹¶è§£æçœŸå®ç‰Œé¢æ•°æ®è¿›è¡Œåˆ¤å®š
- å®Œå…¨ä½¿ç”¨ä½ è¦æ±‚çš„åˆ¤å®šè§„åˆ™ï¼ˆæ”¾æ°´ / ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰ / èƒœç‡ä¸­ç­‰ / æ”¶å‰²ï¼‰
- Telegram é€šçŸ¥ï¼šä»…åœ¨ æ”¾æ°´ æˆ– ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰ æ—¶å¼€å§‹é€šçŸ¥ï¼Œç»“æŸæ—¶å‘ç»“æŸé€šçŸ¥ï¼ˆå«çœŸå®æŒç»­æ—¶é—´ï¼‰
- è¾“å‡º last_run_summary.json ä¾¿äºè°ƒè¯•
"""
import os, time, json, math
from datetime import datetime, timedelta, timezone
from pathlib import Path
from io import BytesIO

import requests
import numpy as np
from PIL import Image
import cv2

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
from sklearn.cluster import KMeans

# ---------------- CONFIG ----------------
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN", "").strip()
TG_CHAT_ID  = os.environ.get("TG_CHAT_ID", "").strip()
DG_LINKS = ["https://dg18.co/wap/", "https://dg18.co/"]

# Visual thresholds
MIN_POINTS_FOR_REAL_BOARD = int(os.environ.get("MIN_POINTS_FOR_REAL_BOARD", "40"))
MAX_WAIT_SECONDS = int(os.environ.get("MAX_WAIT_SECONDS", "30"))
RETRY_ATTEMPTS = int(os.environ.get("RETRY_ATTEMPTS", "2"))

# Logic thresholds
MIN_BOARDS_FOR_PAW = int(os.environ.get("MIN_BOARDS_FOR_PAW", "3"))
MID_LONG_REQ = int(os.environ.get("MID_LONG_REQ", "2"))
COOLDOWN_MINUTES = int(os.environ.get("COOLDOWN_MINUTES", "10"))

STATE_FILE = "state.json"
LAST_SUMMARY = "last_run_summary.json"
TZ = timezone(timedelta(hours=8))

# ---------------- helpers ----------------
def nowstr():
    return datetime.now(TZ).strftime("%Y-%m-%d %H:%M:%S")

def log(msg):
    print(f"[{nowstr()}] {msg}", flush=True)

def send_telegram(text):
    if not TG_BOT_TOKEN or not TG_CHAT_ID:
        log("Telegram æœªé…ç½®ï¼Œè·³è¿‡å‘é€")
        return False
    try:
        r = requests.post(f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage",
                          data={"chat_id":TG_CHAT_ID, "text":text, "parse_mode":"HTML"}, timeout=15)
        j = r.json()
        if j.get("ok"):
            log("Telegram å·²å‘é€")
            return True
        else:
            log(f"Telegram è¿”å›: {j}")
            return False
    except Exception as e:
        log(f"å‘é€ Telegram å¤±è´¥: {e}")
        return False

def load_state():
    if not Path(STATE_FILE).exists():
        return {"active":False,"kind":None,"start_time":None,"last_seen":None,"history":[]}
    return json.loads(Path(STATE_FILE).read_text(encoding="utf-8"))

def save_state(s):
    Path(STATE_FILE).write_text(json.dumps(s, ensure_ascii=False, indent=2), encoding="utf-8")

# ---------------- image utilities ----------------
def pil_from_bytes(b):
    return Image.open(BytesIO(b)).convert("RGB")

def cv_from_pil(pil):
    return cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)

def detect_points(bgr):
    """HSV + Hough fallback åœ†ç‚¹æ£€æµ‹"""
    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)
    # red
    lower1,upper1 = np.array([0,100,80]), np.array([10,255,255])
    lower2,upper2 = np.array([160,100,80]), np.array([179,255,255])
    mask_r = cv2.inRange(hsv, lower1, upper1) | cv2.inRange(hsv, lower2, upper2)
    # blue
    lowerb,upperb = np.array([90,60,50]), np.array([140,255,255])
    mask_b = cv2.inRange(hsv, lowerb, upperb)
    k = np.ones((3,3), np.uint8)
    mask_r = cv2.morphologyEx(mask_r, cv2.MORPH_OPEN, k, iterations=1)
    mask_b = cv2.morphologyEx(mask_b, cv2.MORPH_OPEN, k, iterations=1)
    pts=[]
    for mask,label in [(mask_r,'B'), (mask_b,'P')]:
        contours,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for cnt in contours:
            a = cv2.contourArea(cnt)
            if a < 10: continue
            M = cv2.moments(cnt)
            if M['m00']==0: continue
            cx = int(M['m10']/M['m00']); cy=int(M['m01']/M['m00'])
            pts.append((cx,cy,label))
    # fallback Hough if too few
    if len(pts) < 8:
        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=12, param1=50, param2=18, minRadius=4, maxRadius=18)
        if circles is not None:
            for (x,y,r) in np.round(circles[0,:]).astype("int"):
                px,py = max(0,min(bgr.shape[1]-1,x)), max(0,min(bgr.shape[0]-1,y))
                b,g,rr = bgr[py,px]
                if rr > b+20: lab='B'
                elif b > rr+20: lab='P'
                else: lab='B'
                pts.append((int(x),int(y),lab))
    return pts

# ---------------- clustering and per-table analysis ----------------
def cluster_regions(points, img_w, img_h):
    if not points: return []
    cell = max(60, int(min(img_w,img_h)/12))
    cols = math.ceil(img_w/cell); rows = math.ceil(img_h/cell)
    grid=[[0]*cols for _ in range(rows)]
    for x,y,_ in points:
        cx = min(cols-1, x//cell); cy = min(rows-1, y//cell)
        grid[cy][cx]+=1
    hits=[(r,c) for r in range(rows) for c in range(cols) if grid[r][c]>=6]
    if not hits:
        pts_arr = np.array([[p[0],p[1]] for p in points])
        k = min(8, max(1, len(points)//8))
        try:
            km = KMeans(n_clusters=k, random_state=0).fit(pts_arr)
            regs=[]
            for lab in range(k):
                sel = pts_arr[km.labels_==lab]
                if sel.shape[0]==0: continue
                x0,y0 = sel.min(axis=0); x1,y1 = sel.max(axis=0)
                regs.append((int(max(0,x0-10)), int(max(0,y0-10)), int(min(img_w,x1-x0+20)), int(min(img_h,y1-y0+20))))
            return regs
        except Exception:
            return []
    rects=[]
    for (r,c) in hits:
        x=r*c  # placeholder not used
    # merge adjacent
    rects=[]
    for (r,c) in hits:
        x = c*cell; y=r*cell; w=cell; h=cell
        merged=False
        for i,(rx,ry,rw,rh) in enumerate(rects):
            if not (x>rx+rw+cell or x+w<rx-cell or y>ry+rh+cell or y+h<ry-cell):
                nx=min(rx,x); ny=min(ry,y); nw=max(rx+rw, x+w)-nx; nh=max(ry+rh, y+h)-ny
                rects[i]=(nx,ny,nw,nh); merged=True; break
        if not merged:
            rects.append((x,y,w,h))
    regs=[]
    for (x,y,w,h) in rects:
        nx=max(0,x-10); ny=max(0,y-10); nw=min(img_w-nx,w+20); nh=min(img_h-ny,h+20)
        regs.append((int(nx),int(ny),int(nw),int(nh)))
    return regs

def analyze_region(bgr, region):
    x,y,w,h = region
    crop = bgr[y:y+h, x:x+w]
    pts = detect_points(crop)
    if not pts:
        return {"total":0,"maxRun":0,"category":"empty","runs":[],"multiRuns":0,"cols_max":[],"consec3":False}
    pts_sorted = sorted(pts, key=lambda p: p[0])
    xs = [p[0] for p in pts_sorted]
    col_groups=[]
    for i,xv in enumerate(xs):
        placed=False
        for grp in col_groups:
            gx = sum(pts_sorted[j][0] for j in grp)/len(grp)
            if abs(gx - xv) <= max(8, w//45):
                grp.append(i); placed=True; break
        if not placed:
            col_groups.append([i])
    sequences=[]; cols_max=[]
    for grp in col_groups:
        col_pts = sorted([pts_sorted[i] for i in grp], key=lambda t:t[1])
        seq = [p[2] for p in col_pts]
        sequences.append(seq)
        # per-column max run
        m=0
        if seq:
            cur=seq[0]; ln=1
            for s in seq[1:]:
                if s==cur: ln+=1
                else:
                    m=max(m,ln); cur=s; ln=1
            m=max(m,ln)
        cols_max.append(m)
    flattened=[]; maxlen = max((len(s) for s in sequences), default=0)
    for r in range(maxlen):
        for col in sequences:
            if r < len(col):
                flattened.append(col[r])
    runs=[]
    if flattened:
        cur=flattened[0]; ln=1
        for t in flattened[1:]:
            if t==cur: ln+=1
            else:
                runs.append({"color":cur,"len":ln}); cur=t; ln=1
        runs.append({"color":cur,"len":ln})
    maxRun = max((r["len"] for r in runs), default=0)
    cat="other"
    if maxRun>=10: cat="super_long"
    elif maxRun>=8: cat="long"
    elif maxRun>=4: cat="longish"
    elif maxRun==1: cat="single"
    multiRuns = sum(1 for r in runs if r["len"]>=4)
    consec3=False
    if len(cols_max) >= 3:
        for i in range(len(cols_max)-2):
            if cols_max[i] >=4 and cols_max[i+1] >=4 and cols_max[i+2] >=4:
                consec3=True; break
    return {"total":len(flattened),"maxRun":maxRun,"category":cat,"runs":runs,"multiRuns":multiRuns,"cols_max":cols_max,"consec3":consec3}

def classify_overall(board_stats):
    longCount = sum(1 for b in board_stats if b['category'] in ('long','super_long'))
    superCount = sum(1 for b in board_stats if b['category']=='super_long')
    consec3_count = sum(1 for b in board_stats if b.get('consec3',False))
    # ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰ first: >=3 tables have consec3 AND >=2 tables long/ultra
    if consec3_count >= 3 and longCount >= MID_LONG_REQ:
        return "ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰", longCount, superCount
    # æ”¾æ°´ï¼š >= MIN_BOARDS_FOR_PAW longCount
    if longCount >= MIN_BOARDS_FOR_PAW:
        return "æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", longCount, superCount
    totals = [b['total'] for b in board_stats]
    sparse = sum(1 for t in totals if t < 6)
    if len(board_stats)>0 and sparse >= len(board_stats)*0.6:
        return "èƒœç‡è°ƒä½ / æ”¶å‰²æ—¶æ®µ", longCount, superCount
    return "èƒœç‡ä¸­ç­‰ï¼ˆå¹³å°æ”¶å‰²ä¸­ç­‰æ—¶æ®µï¼‰", longCount, superCount

# ---------------- Playwright with network capture ----------------
def try_visit_and_capture(play, url):
    """
    å°è¯•æ‰“å¼€ url, ç‚¹å‡» Free, æ¨¡æ‹Ÿæ»‘åŠ¨, åŒæ—¶æ•è· network responses (xhr/fetch)
    è¿”å› dict:
      {"mode":"visual","screenshot":bytes, "points":int}
      æˆ– {"mode":"network","api_candidates":[...parsed...]}
      æˆ– None
    """
    browser = play.chromium.launch(headless=True, args=["--no-sandbox","--disable-gpu","--disable-dev-shm-usage"])
    try:
        context = browser.new_context(viewport={"width":1366,"height":768})
        # reduce webdriver flag
        context.add_init_script("() => { Object.defineProperty(navigator, 'webdriver', {get: () => false}); }")
        responses = []
        def on_response(resp):
            try:
                ct = resp.headers.get("content-type","")
                if resp.request.resource_type in ("xhr","fetch") or "json" in ct or "/api/" in resp.url:
                    # read body safely
                    try:
                        text = resp.text()
                        responses.append({"url":resp.url, "status":resp.status, "text": text})
                    except Exception:
                        pass
            except Exception:
                pass
        context.on("response", on_response)
        page = context.new_page()
        page.set_default_timeout(20000)
        log(f"æ‰“å¼€ {url}")
        page.goto(url, timeout=20000)
        time.sleep(1.0)
        # attempt click text Free variants
        clicked=False
        for txt in ["Free","å…è´¹è¯•ç©","å…è´¹","Play Free","è¯•ç©","è¿›å…¥","Play"]:
            try:
                els = page.locator(f"text={txt}")
                if els.count() > 0:
                    try:
                        els.first.click(timeout=3000); clicked=True; log(f"ç‚¹å‡» {txt}"); break
                    except Exception:
                        try:
                            page.evaluate("(e)=>e.click()", els.first); clicked=True; log(f"JS ç‚¹å‡» {txt}"); break
                        except Exception:
                            pass
            except Exception:
                continue
        if not clicked:
            try:
                btn = page.query_selector("button")
                if btn:
                    btn.click(timeout=2000); clicked=True; log("ç‚¹å‡»ç¬¬ä¸€ä¸ª button fallback")
            except Exception:
                pass
        time.sleep(0.8)
        # try drag slider on page (simple)
        try:
            vp = page.viewport_size or {"width":1366,"height":768}
            sx = vp['width']*0.25; sy = vp['height']*0.6; ex = vp['width']*0.75
            page.mouse.move(sx, sy); page.mouse.down(); page.mouse.move(ex, sy, steps=30); page.mouse.up()
            log("é¡µé¢å±‚çº§æ‹–åŠ¨å°è¯•")
        except Exception as e:
            log(f"æ‹–åŠ¨å°è¯•å¼‚å¸¸: {e}")

        # wait loop: try until we see MIN_POINTS_FOR_REAL_BOARD or timeout
        start = time.time()
        last_shot = None
        while time.time() - start < MAX_WAIT_SECONDS:
            try:
                shot = page.screenshot(full_page=True)
                last_shot = shot
                pil = pil_from_bytes(shot)
                bgr = cv_from_pil(pil)
                pts = detect_points(bgr)
                log(f"ç­‰å¾…æ£€æµ‹: å½“å‰å½©ç‚¹={len(pts)} (é˜ˆå€¼ {MIN_POINTS_FOR_REAL_BOARD})")
                if len(pts) >= MIN_POINTS_FOR_REAL_BOARD:
                    return {"mode":"visual","screenshot":shot,"points":len(pts)}
            except Exception as e:
                log(f"æˆªå›¾æˆ–æ£€æµ‹å¼‚å¸¸: {e}")
            time.sleep(1.2)
        # è¶…æ—¶ï¼šå°è¯•è§£ææ•è·åˆ°çš„ responses
        log("è§†è§‰ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•è§£ææ•è·åˆ°çš„ç½‘ç»œå“åº”")
        parsed = parse_network_responses_for_boards(responses)
        if parsed:
            return {"mode":"network","api_candidates":parsed}
        # æœ€åä»è¿”å›æœ€åæˆªå›¾ï¼ˆè§†ä¸ºæœªè¿›å…¥å®ç›˜ï¼‰
        if last_shot:
            pil = pil_from_bytes(last_shot)
            bgr = cv_from_pil(pil)
            pts = detect_points(bgr)
            return {"mode":"visual","screenshot":last_shot,"points":len(pts)}
        return None
    finally:
        try:
            browser.close()
        except:
            pass

# ---------------- parsing network JSON heuristics ----------------
def find_lists_in_obj(obj):
    """é€’å½’æ‰¾å‡ºå€™é€‰çš„ listï¼ˆé•¿åº¦>3 ä¸”å…ƒç´ ä¸º dict æˆ– listï¼‰"""
    candidates=[]
    if isinstance(obj, list):
        if len(obj) >= 4:
            candidates.append(obj)
        for item in obj:
            candidates.extend(find_lists_in_obj(item))
    elif isinstance(obj, dict):
        for v in obj.values():
            candidates.extend(find_lists_in_obj(v))
    return candidates

def parse_sequence_from_item(item):
    """
    ç»™å®šä¸€ä¸ª dict/listï¼Œå°è¯•æŠ½å– B/P æˆ– banker/player æˆ– 0/1 åºåˆ—
    è¿”å› list of 'B'/'P' strings æˆ– None
    """
    seq = []
    if isinstance(item, list):
        for sub in item:
            res = parse_sequence_from_item(sub)
            if res:
                seq.extend(res if isinstance(res,list) else [res])
        if seq:
            return seq
    elif isinstance(item, dict):
        # common fields
        lower_keys = {k.lower():v for k,v in item.items()}
        # check winner-like fields
        for k in ("winner","result","outcome","type","side","hand","banker","player"):
            if k in lower_keys:
                v = lower_keys[k]
                if isinstance(v, str):
                    if v.lower().startswith("b") or "bank" in v.lower():
                        return ["B"]
                    if v.lower().startswith("p") or "player" in v.lower() or "é—²" in v:
                        return ["P"]
                if isinstance(v, (int,float)):
                    # sometimes 1/2 mapping â€” guess: 1 banker, 2 player
                    if int(v) == 1: return ["B"]
                    if int(v) == 2: return ["P"]
        # if dict contains a list of moves
        for v in item.values():
            res = parse_sequence_from_item(v)
            if res:
                return res
    return None

def parse_network_responses_for_boards(responses):
    """
    responses: list of {"url":..., "status":..., "text":...}
    è¿”å› parsed_boards: list of per-board sequences (each is list of 'B'/'P')
    """
    parsed_boards = []
    raw_candidates = []
    for r in responses:
        text = r.get("text","")
        if not text: continue
        # try json load
        try:
            j = json.loads(text)
        except Exception:
            # sometimes text includes JSON in HTML - skip
            continue
        lists = find_lists_in_obj(j)
        for lst in lists:
            # try parse each element into 'B'/'P' sequence
            seq = []
            for item in lst:
                res = parse_sequence_from_item(item)
                if res:
                    if isinstance(res,list): seq.extend(res)
                    else: seq.append(res)
            # if we get a lot of B/P markers, keep
            if len(seq) >= 8:
                parsed_boards.append(seq)
                raw_candidates.append({"url": r.get("url"), "sample": lst[:6]})
    # Post-process: if parsed_boards empty, try to find arrays of simple tokens
    if not parsed_boards:
        for r in responses:
            text = r.get("text","")
            if not text: continue
            # look for patterns like ["B","P","B",...]
            try:
                j = json.loads(text)
                if isinstance(j, list) and all(isinstance(x,str) for x in j) and len(j)>=8:
                    # normalize tokens
                    seq = []
                    for x in j:
                        s = x.strip().upper()
                        if s.startswith("B") or "BANK" in s.upper() or s=="åº„": seq.append("B")
                        elif s.startswith("P") or "PLAY" in s.upper() or s=="é—²": seq.append("P")
                    if len(seq)>=8:
                        parsed_boards.append(seq)
                        raw_candidates.append({"url": r.get("url"), "sample": j[:8]})
            except Exception:
                pass
    # return parsed_boards as list of sequences
    if parsed_boards:
        return {"parsed": parsed_boards, "raw_candidates": raw_candidates}
    return None

# ---------------- convert sequences into board_stats (compatible with visual) ----------------
def boards_from_sequences(seq_lists):
    """seq_lists: list of sequences of 'B'/'P' -> produce board_stats list"""
    boards = []
    for seq in seq_lists:
        # split into columns like visual flattening assumption: attempt 6 columns
        # heuristic: chunk into columns of length ~ ceil(len/6)
        n = len(seq)
        cols = 6
        col_h = math.ceil(n/cols)
        columns = []
        for c in range(cols):
            start = c*col_h; end = start+col_h
            col = seq[start:end]
            if col:
                columns.append(col)
        # flattened
        flattened = []
        maxlen = max((len(col) for col in columns), default=0)
        for r in range(maxlen):
            for col in columns:
                if r < len(col): flattened.append(col[r])
        # runs
        runs=[]
        if flattened:
            cur=flattened[0]; ln=1
            for t in flattened[1:]:
                if t==cur: ln+=1
                else:
                    runs.append({"color":cur,"len":ln}); cur=t; ln=1
            runs.append({"color":cur,"len":ln})
        maxRun = max((r["len"] for r in runs), default=0)
        cat="other"
        if maxRun>=10: cat="super_long"
        elif maxRun>=8: cat="long"
        elif maxRun>=4: cat="longish"
        elif maxRun==1: cat="single"
        multiRuns = sum(1 for r in runs if r["len"]>=4)
        # detect consec3 in columns
        cols_max = []
        for col in columns:
            m=0
            if col:
                cur=col[0]; ln=1
                for s in col[1:]:
                    if s==cur: ln+=1
                    else:
                        m=max(m,ln); cur=s; ln=1
                m=max(m,ln)
            cols_max.append(m)
        consec3=False
        if len(cols_max) >= 3:
            for i in range(len(cols_max)-2):
                if cols_max[i]>=4 and cols_max[i+1]>=4 and cols_max[i+2]>=4:
                    consec3=True; break
        boards.append({"total":len(flattened),"maxRun":maxRun,"category":cat,"runs":runs,"multiRuns":multiRuns,"cols_max":cols_max,"consec3":consec3})
    return boards

# ---------------- main flow ----------------
def main():
    log("å¼€å§‹æ£€æµ‹å¾ªç¯ï¼ˆæœ€ç»ˆç‰ˆï¼‰")
    state = load_state()
    result=None
    with sync_playwright() as p:
        for url in DG_LINKS:
            try:
                result = try_visit_and_capture(p, url)
                if result:
                    break
            except Exception as e:
                log(f"è®¿é—® {url} å¼‚å¸¸: {e}")
                continue
    summary = {"ts": nowstr(), "mode": None, "info": None, "boards":[]}
    overall=None; longCount=0; superCount=0
    # handle result
    if not result:
        log("æœªèƒ½é€šè¿‡ä»»ä½•æ–¹å¼è·å¾—æ•°æ®")
        save_state(state)
        summary["mode"]="none"
        Path(LAST_SUMMARY).write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
        return

    if result.get("mode") == "visual":
        pts = result.get("points", 0)
        summary["mode"]="visual"
        summary["points"]=pts
        log(f"è§†è§‰æ¨¡å¼ï¼šæˆªå›¾å½©ç‚¹={pts}")
        # if we have screenshot and enough points do visual analysis
        if pts >= 8:
            pil = pil_from_bytes(result["screenshot"])
            bgr = cv_from_pil(pil)
            regions = cluster_regions(detect_points(bgr), bgr.shape[1], bgr.shape[0])
            # fallback: if cluster_regions returned empty, build single region covering entire
            if not regions:
                regions=[(0,0,bgr.shape[1], bgr.shape[0])]
            boards=[]
            for r in regions:
                st = analyze_region(bgr, r)
                boards.append(st)
            overall, longCount, superCount = classify_overall(boards)
            summary["boards"]=boards
        else:
            # too few points to be reliable; mark as failed visual
            summary["info"]="è§†è§‰æˆªå›¾ç‚¹æ•°è¿‡å°‘"
            # try network parsing? (we already attempted in try_visit)
            summary["boards"]=[]
            overall="èƒœç‡ä¸­ç­‰ï¼ˆå¹³å°æ”¶å‰²ä¸­ç­‰æ—¶æ®µï¼‰"
    elif result.get("mode") == "network":
        summary["mode"]="network"
        parsed = result.get("api_candidates", {})
        summary["raw_candidates"] = parsed.get("raw_candidates", [])[:6]
        seqs = parsed.get("parsed", [])
        boards = boards_from_sequences(seqs)
        summary["boards"]=boards
        overall, longCount, superCount = classify_overall(boards)
        log(f"ç½‘ç»œæ¨¡å¼è§£æåˆ° {len(boards)} æ¡Œï¼Œåˆ¤å®š {overall}")
    else:
        summary["mode"]="unknown"
        summary["info"]="æœªçŸ¥ç»“æœ"
    # state transitions and Telegram
    now_iso = datetime.now(TZ).isoformat()
    was_active = state.get("active", False)
    is_active_now = overall in ("æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", "ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰")
    if is_active_now and not was_active:
        # start
        history = state.get("history", [])
        durations = [h.get("duration_minutes",0) for h in history if h.get("duration_minutes",0)>0]
        est = round(sum(durations)/len(durations)) if durations else 10
        est_end = (datetime.now(TZ) + timedelta(minutes=est)).strftime("%Y-%m-%d %H:%M")
        emoji="ğŸš©"
        msg = f"{emoji} <b>DG æé†’ â€” {overall}</b>\nåµæ¸¬æ™‚é–“ (MYT): {now_iso}\né•·/è¶…é•¿é¾™æ¡Œæ•¸={longCount}ï¼Œè¶…é•¿é¾™={superCount}\nä¼°è¨ˆçµæŸæ™‚é–“: {est_end}ï¼ˆç´„ {est} åˆ†é˜ï¼‰"
        send_telegram(msg)
        state = {"active":True,"kind":overall,"start_time":now_iso,"last_seen":now_iso,"history":state.get("history", [])}
        save_state(state)
    elif is_active_now and was_active:
        state["last_seen"]=now_iso; state["kind"]=overall; save_state(state)
    elif (not is_active_now) and was_active:
        start = datetime.fromisoformat(state.get("start_time"))
        end = datetime.now(TZ); dur = round((end-start).total_seconds()/60)
        entry = {"kind": state.get("kind"), "start_time": state.get("start_time"), "end_time": end.isoformat(), "duration_minutes": dur}
        hist = state.get("history", []); hist.append(entry); hist = hist[-120:]
        new_state = {"active":False,"kind":None,"start_time":None,"last_seen":None,"history":hist}
        save_state(new_state)
        emoji="âœ…"
        msg = f"{emoji} <b>DG æé†’ â€” {state.get('kind')} å·²çµæŸ</b>\né–‹å§‹: {entry['start_time']}\nçµæŸ: {entry['end_time']}\nå¯¦éš›æŒçºŒ: {dur} åˆ†é˜"
        send_telegram(msg)
    else:
        save_state(state)
    # write summary
    summary["overall"]= overall
    summary["longCount"]= longCount
    summary["superCount"]= superCount
    summary["ts"]= nowstr()
    Path(LAST_SUMMARY).write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
    log("å†™å…¥ last_run_summary.json")
    return

# small wrappers
def pil_from_bytes(b): return Image.open(BytesIO(b)).convert("RGB")
def cv_from_pil(pil): return cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"ä¸»ç¨‹å¼å¼‚å¸¸: {e}")
        raise
