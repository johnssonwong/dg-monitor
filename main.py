# main.py
# DG å®ç›˜æ£€æµ‹ + ç«‹å³å†å²æ›¿è¡¥ (Wayback / å…¬å…±API) -> Telegram é€šçŸ¥
# ä¿å­˜æ–‡ä»¶åè¯·ä¿æŒä¸º main.py ï¼ˆä¸è¦æ”¹åï¼‰
# è¯´æ˜ï¼ˆå¿…é¡»é˜…è¯»ï¼‰ï¼š
# 1) è¯·åœ¨è¿è¡Œç¯å¢ƒä¸­è®¾ç½®ç¯å¢ƒå˜é‡ TG_BOT_TOKEN ä¸ TG_CHAT_IDï¼ˆæˆ–åœ¨ä»“åº“ Secrets é‡Œè®¾ç½®ï¼‰ã€‚
# 2) è„šæœ¬å…ˆå°è¯•è¿›å…¥ DG å®ç›˜ï¼›è¿›å…¥å¤±è´¥ä¼šç«‹å³å¯ç”¨å†å²æ›¿è¡¥ï¼ˆWayback + å…¬å…± APIï¼‰ã€‚
# 3) è‹¥å†å²æ•°æ®åˆ¤å®šä¸º"æ”¾æ°´"æˆ–"ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰"ï¼Œè„šæœ¬ä¼šé©¬ä¸Šå‘é€ Telegram æ¶ˆæ¯å¹¶è®°å½•å†å²ã€‚
# 4) è‹¥ä½ å¸Œæœ›è„šæœ¬æŠŠ Token / ChatID è‡ªåŠ¨å†™å…¥ï¼Œè¯·åœ¨ repo secrets / ç¯å¢ƒå˜é‡ä¸­é…ç½®ï¼Œä¸è¦æŠŠ Token è´´åˆ°å…¬å¼€ä½ç½®ã€‚

import os, sys, time, json, random, traceback
from datetime import datetime, timedelta, timezone
from io import BytesIO
import requests
import numpy as np
from PIL import Image
import cv2
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout

# ------------------- é…ç½®åŒºï¼ˆå¯è°ƒï¼‰ -------------------
DG_LINKS = ["https://dg18.co/wap/", "https://dg18.co/"]   # ç›®æ ‡ DG é“¾æ¥ï¼ˆä¿ç•™ï¼‰
TG_TOKEN_ENV = "TG_BOT_TOKEN"
TG_CHAT_ENV = "TG_CHAT_ID"

# åˆ¤å®šé˜ˆå€¼ï¼ˆå¯åœ¨ç¯å¢ƒå˜é‡å¾®è°ƒï¼‰
MIN_POINTS_FOR_REAL = int(os.environ.get("MIN_POINTS_FOR_REAL", "40"))
DILATE_KERNEL_SIZE = int(os.environ.get("DILATE_KERNEL_SIZE", "40"))
MIN_BOARDS_FOR_PAW = int(os.environ.get("MIN_BOARDS_FOR_PAW", "3"))  # æ”¾æ°´åˆ¤å®šï¼šè‡³å°‘å‡ ä¸ªé•¿é¾™/è¶…é•¿é¾™
HISTORY_LOOKBACK_DAYS = int(os.environ.get("HISTORY_LOOKBACK_DAYS", "28"))
MIN_HISTORY_EVENTS_FOR_PRED = int(os.environ.get("MIN_HISTORY_EVENTS_FOR_PRED", "3"))
PRED_BUCKET_MINUTES = int(os.environ.get("PRED_BUCKET_MINUTES", "15"))
PRED_LEAD_MINUTES = int(os.environ.get("PRED_LEAD_MINUTES", "10"))
WAYBACK_MAX_SNAPSHOTS = int(os.environ.get("WAYBACK_MAX_SNAPSHOTS","40"))
WAYBACK_RATE_SLEEP = float(os.environ.get("WAYBACK_RATE_SLEEP","1.2"))

STATE_FILE = "state.json"
SUMMARY_FILE = "last_summary.json"
LAST_SCREENSHOT = "last_screenshot.png"

TZ = timezone(timedelta(hours=8))  # Malaysia UTC+8
# ----------------------------------------------------

def now_tz(): return datetime.now(TZ)
def nowstr(): return now_tz().strftime("%Y-%m-%d %H:%M:%S")
def log(s): print(f"[{nowstr()}] {s}", flush=True)

# ---------------- Telegram ----------------
def send_telegram(text):
    token = os.environ.get(TG_TOKEN_ENV, "").strip()
    chat = os.environ.get(TG_CHAT_ENV, "").strip()
    if not token or not chat:
        log("Telegram credentials not set. è¯·åœ¨è¿è¡Œç¯å¢ƒä¸­è®¾ç½® TG_BOT_TOKEN ä¸ TG_CHAT_IDã€‚")
        return False
    try:
        r = requests.post(f"https://api.telegram.org/bot{token}/sendMessage",
                          data={"chat_id": chat, "text": text}, timeout=20)
        j = r.json()
        if j.get("ok"):
            log("Telegram å·²å‘é€")
            return True
        else:
            log(f"Telegram è¿”å›é ok: {j}")
            return False
    except Exception as e:
        log(f"å‘é€ Telegram å¼‚å¸¸: {e}")
        return False

# ---------------- state ----------------
def load_state():
    if not os.path.exists(STATE_FILE):
        return {"active":False,"kind":None,"start_time":None,"last_seen":None,"history":[]}
    try:
        with open(STATE_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {"active":False,"kind":None,"start_time":None,"last_seen":None,"history":[]}

def save_state(s):
    with open(STATE_FILE,"w",encoding="utf-8") as f:
        json.dump(s,f,ensure_ascii=False,indent=2)

# ---------------- Image helpers ----------------
def pil_from_bytes(b): return Image.open(BytesIO(b)).convert("RGB")
def cv_from_pil(p): return cv2.cvtColor(np.array(p), cv2.COLOR_RGB2BGR)

def detect_color_points(bgr):
    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)
    # red approximate
    lower1,upper1 = np.array([0,90,60]), np.array([10,255,255])
    lower2,upper2 = np.array([160,90,60]), np.array([179,255,255])
    mask_r = cv2.inRange(hsv, lower1, upper1) | cv2.inRange(hsv, lower2, upper2)
    # blue approximate
    lowerb,upperb = np.array([85,60,40]), np.array([140,255,255])
    mask_b = cv2.inRange(hsv, lowerb, upperb)
    kernel = np.ones((3,3), np.uint8)
    mask_r = cv2.morphologyEx(mask_r, cv2.MORPH_OPEN, kernel, iterations=1)
    mask_b = cv2.morphologyEx(mask_b, cv2.MORPH_OPEN, kernel, iterations=1)
    points=[]
    for mask,label in [(mask_r,'B'),(mask_b,'P')]:
        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for cnt in cnts:
            area = cv2.contourArea(cnt)
            if area < 8: continue
            M = cv2.moments(cnt)
            if M.get("m00",0)==0: continue
            cx = int(M["m10"]/M["m00"]); cy = int(M["m01"]/M["m00"])
            points.append((cx,cy,label))
    return points, mask_r, mask_b

def cluster_points_to_boards(points, img_shape):
    h,w = img_shape[:2]
    mask = np.zeros((h,w), dtype=np.uint8)
    for x,y,_ in points:
        if 0<=x<w and 0<=y<h: mask[y,x] = 255
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (DILATE_KERNEL_SIZE,DILATE_KERNEL_SIZE))
    big = cv2.dilate(mask, kernel, iterations=1)
    num, labels, stats, _ = cv2.connectedComponentsWithStats(big, connectivity=8)
    rects=[]
    for i in range(1,num):
        x,y,w_,h_ = stats[i,cv2.CC_STAT_LEFT], stats[i,cv2.CC_STAT_TOP], stats[i,cv2.CC_STAT_WIDTH], stats[i,cv2.CC_STAT_HEIGHT]
        if w_ < 60 or h_ < 40: continue
        pad=8
        x0=max(0,x-pad); y0=max(0,y-pad); x1=min(w-1,x+w_+pad); y1=min(h-1,y+h_+pad)
        rects.append((x0,y0,x1-x0,y1-y0))
    if not rects:
        # fallback grid
        cols = max(3, w//300); rows = max(2, h//200)
        cw = w//cols; ch = h//rows
        for r in range(rows):
            for c in range(cols):
                rects.append((c*cw, r*ch, cw, ch))
    return rects

def analyze_board(bgr, rect):
    x,y,w,h = rect
    crop = bgr[y:y+h, x:x+w]
    pts,_,_ = detect_color_points(crop)
    pts_local = [(px,py,c) for (px,py,c) in pts]
    if not pts_local:
        return {"total":0,"maxRun":0,"category":"empty","columns":[],"runs":[]}
    xs = [p[0] for p in pts_local]
    idx_sorted = sorted(range(len(xs)), key=lambda i: xs[i])
    col_groups=[]
    for idx in idx_sorted:
        xv = xs[idx]; placed=False
        for g in col_groups:
            gxs = [pts_local[i][0] for i in g]
            if abs(np.mean(gxs) - xv) <= max(10, w//40):
                g.append(idx); placed=True; break
        if not placed:
            col_groups.append([idx])
    columns=[]
    for g in col_groups:
        col_pts = sorted([pts_local[i] for i in g], key=lambda t: t[1])
        columns.append([p[2] for p in col_pts])
    # build flattened row-wise for runs
    flattened=[]
    maxlen = max((len(c) for c in columns), default=0)
    for r in range(maxlen):
        for col in columns:
            if r < len(col): flattened.append(col[r])
    runs=[]
    if flattened:
        cur={'color':flattened[0],'len':1}
        for k in range(1,len(flattened)):
            if flattened[k]==cur['color']: cur['len']+=1
            else:
                runs.append(cur); cur={'color':flattened[k],'len':1}
        runs.append(cur)
    maxRun = max((r['len'] for r in runs), default=0)
    cat = "other"
    if maxRun >= 10: cat = "super_long"
    elif maxRun >= 8: cat = "long"
    elif maxRun >= 4: cat = "longish"
    elif maxRun == 1: cat = "single"
    return {"total": len(flattened), "maxRun": maxRun, "category": cat, "columns": columns, "runs": runs}

# ---------------- Classification ----------------
def classify_overall(board_infos):
    longCount = sum(1 for b in board_infos if b['category'] in ('long','super_long'))
    superCount = sum(1 for b in board_infos if b['category']=='super_long')
    # check multi-column è¿ç  (è¿ç»­ 3 æ’ æ¯åˆ—å‡ >=4)
    def board_has_3consec_multicolumn(columns):
        col_runlens=[]
        for col in columns:
            if not col:
                col_runlens.append(0); continue
            ccur=col[0]; clen=1; maxc=1
            for t in col[1:]:
                if t==ccur: clen+=1
                else:
                    if clen>maxc: maxc=clen
                    ccur=t; clen=1
            if clen>maxc: maxc=clen
            col_runlens.append(maxc)
        for i in range(len(col_runlens)-2):
            if col_runlens[i] >=4 and col_runlens[i+1] >=4 and col_runlens[i+2] >=4:
                return True
        return False
    boards_with_multicol = sum(1 for b in board_infos if board_has_3consec_multicolumn(b['columns']))
    boards_with_long = sum(1 for b in board_infos if b['maxRun'] >= 8)
    # åˆ¤å®šæ”¾æ°´ï¼ˆæé«˜èƒœç‡ï¼‰
    if longCount >= MIN_BOARDS_FOR_PAW:
        return "æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", longCount, superCount
    # åˆ¤å®šä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰
    if boards_with_multicol >= 3 and boards_with_long >= 2:
        return "ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰", boards_with_long, sum(1 for b in board_infos if b['category']=='super_long')
    totals = [b['total'] for b in board_infos]
    if board_infos and sum(1 for t in totals if t < 6) >= len(board_infos)*0.6:
        return "èƒœç‡è°ƒä½ / æ”¶å‰²æ—¶æ®µ", sum(1 for b in board_infos if b['maxRun']>=8), sum(1 for b in board_infos if b['category']=='super_long')
    return "èƒœç‡ä¸­ç­‰ï¼ˆå¹³å°æ”¶å‰²ä¸­ç­‰æ—¶æ®µï¼‰", sum(1 for b in board_infos if b['maxRun']>=8), sum(1 for b in board_infos if b['category']=='super_long')

# ------------- Playwright helpers -------------
def apply_stealth(page):
    page.add_init_script("""
    Object.defineProperty(navigator, 'webdriver', {get: () => false});
    Object.defineProperty(navigator, 'languages', {get: () => ['en-US','en']});
    Object.defineProperty(navigator, 'plugins', {get: () => [1,2,3,4]});
    window.chrome = { runtime: {} };
    """)

def human_like_drag(page, start_x, start_y, end_x, end_y, steps=30):
    page.mouse.move(start_x, start_y)
    page.mouse.down()
    for i in range(1, steps+1):
        nx = start_x + (end_x - start_x) * (i/steps) + random.uniform(-2,2)
        ny = start_y + (end_y - start_y) * (i/steps) + random.uniform(-1,1)
        page.mouse.move(nx, ny, steps=1)
        time.sleep(random.uniform(0.01, 0.04))
    page.mouse.up()

def try_solve_slider(page):
    # å¤šç­–ç•¥å°è¯•æ»‘åŠ¨å®‰å…¨æ¡
    try:
        selectors = ["input[type=range]","div[role=slider]","div[class*=slider]","div[class*=captcha]","div[class*=slide]"]
        for sel in selectors:
            try:
                els = page.query_selector_all(sel)
                if els and len(els)>0:
                    box = els[0].bounding_box()
                    if box:
                        x0 = box["x"]+2; y0 = box["y"] + box["height"]/2
                        x1 = box["x"] + box["width"] - 6
                        human_like_drag(page, x0, y0, x1, y0, steps=30)
                        time.sleep(1.0)
                        return True
            except Exception:
                continue
        # å›¾åƒè¾…åŠ©æ‰¾ slider
        ss = page.screenshot(full_page=True)
        img = pil_from_bytes(ss); bgr = cv_from_pil(img)
        H,W = bgr.shape[:2]
        region = bgr[int(H*0.25):int(H*0.75), int(W*0.05):int(W*0.95)]
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)
        _,th = cv2.threshold(gray,200,255,cv2.THRESH_BINARY)
        cnts,_ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        best=None; best_area=0
        for cnt in cnts:
            bx,by,bw,bh = cv2.boundingRect(cnt)
            area = bw*bh
            if area > best_area and bw>40 and bw>3*bh:
                best=(bx,by,bw,bh); best_area=area
        if best:
            bx,by,bw,bh = best
            px = int(W*0.05) + bx; py = int(H*0.25) + by
            start_x = px + 6; start_y = py + bh//2; end_x = px + bw - 6
            human_like_drag(page, start_x, start_y, end_x, start_y, steps=30)
            time.sleep(1.2)
            return True
    except Exception as e:
        log(f"try_solve_slider å¼‚å¸¸: {e}")
    return False

def capture_dg_page(attempts=3):
    with sync_playwright() as p:
        user_agents = [
            "Mozilla/5.0 (Linux; Android 12; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        viewports = [(390,844),(1280,900)]
        for attempt in range(attempts):
            ua = random.choice(user_agents); vw,vh = random.choice(viewports)
            browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
            context = browser.new_context(user_agent=ua, viewport={"width":vw,"height":vh}, locale="en-US")
            page = context.new_page(); apply_stealth(page)
            time.sleep(random.uniform(0.3,1.0))
            for url in DG_LINKS:
                try:
                    log(f"æ‰“å¼€ {url}ï¼ˆå°è¯• {attempt+1}ï¼‰")
                    page.goto(url, timeout=35000)
                    time.sleep(1.0 + random.random())
                    clicked=False
                    for txt in ["Free","å…è´¹è¯•ç©","å…è´¹","Play Free","è¯•ç©","Free Play","å…è´¹ä½“éªŒ"]:
                        try:
                            loc = page.locator(f"text={txt}")
                            if loc.count()>0:
                                loc.first.click(timeout=4000); clicked=True; log(f"ç‚¹å‡»æ–‡æœ¬: {txt}"); break
                        except Exception:
                            continue
                    if not clicked:
                        try:
                            els = page.query_selector_all("a,button")
                            for i in range(min(80,len(els))):
                                try:
                                    t = els[i].inner_text().strip().lower()
                                    if "free" in t or "è¯•ç©" in t or "å…è´¹" in t:
                                        els[i].click(timeout=3000); clicked=True; log("ç‚¹å‡»å€™é€‰ a/button"); break
                                except Exception:
                                    continue
                        except Exception:
                            pass
                    time.sleep(0.8 + random.random())
                    # å°è¯•æ»‘å—å¤šæ¬¡
                    slider_ok=False
                    for s in range(6):
                        got = try_solve_slider(page)
                        log(f"slider å°è¯• {s+1} -> {got}")
                        if got:
                            slider_ok=True; break
                        else:
                            try:
                                page.mouse.wheel(0,300); time.sleep(0.6)
                            except Exception:
                                pass
                    # ç­‰å¾…/æ£€æµ‹è¶³å¤Ÿçš„ç‚¹
                    for chk in range(8):
                        ss = page.screenshot(full_page=True)
                        try:
                            with open(LAST_SCREENSHOT,"wb") as f: f.write(ss)
                        except: pass
                        img = pil_from_bytes(ss); bgr = cv_from_pil(img)
                        pts,_,_ = detect_color_points(bgr)
                        log(f"æ£€æµ‹è½® {chk+1}: ç‚¹æ•° {len(pts)}")
                        if len(pts) >= MIN_POINTS_FOR_REAL:
                            log("åˆ¤æ–­ä¸ºå·²è¿›å…¥å®ç›˜ï¼ˆç‚¹æ•°è¶³ï¼‰ã€‚")
                            context.close(); browser.close()
                            return ss
                        time.sleep(1.2 + random.random())
                except PWTimeout as e:
                    log(f"æ‰“å¼€è¶…æ—¶: {e}")
                except Exception as e:
                    log(f"ä¸é¡µé¢äº¤äº’å¼‚å¸¸: {e}")
            try:
                context.close()
            except: pass
            try:
                browser.close()
            except: pass
            time.sleep(2 + random.random())
        log("å¤šæ¬¡å°è¯•åæœªèƒ½è¿›å…¥å®ç›˜")
        return None

# ---------------- Wayback / å†å²æ›¿è¡¥ ----------------
def get_wayback_snapshots(url, from_date=None, to_date=None, limit=40):
    base = "http://web.archive.org/cdx/search/cdx"
    params = {"url": url, "output": "json", "filter": "statuscode:200", "limit": str(limit)}
    if from_date: params["from"] = from_date
    if to_date: params["to"] = to_date
    try:
        r = requests.get(base, params=params, timeout=12)
        if r.status_code != 200:
            log(f"Wayback CDX è¿”å› {r.status_code} for {url}")
            return []
        j = r.json()
        if not j or len(j) < 2: return []
        rows = j[1:]
        timestamps = [row[1] for row in rows if len(row) > 1]
        return timestamps
    except Exception as e:
        log(f"Wayback CDX å¼‚å¸¸: {e}")
        return []

def fetch_wayback_snapshot_and_screenshot(snapshot_ts, target_url):
    snap = f"https://web.archive.org/web/{snapshot_ts}/{target_url}"
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
            context = browser.new_context(viewport={"width":1280,"height":900})
            page = context.new_page()
            page.goto(snap, timeout=30000)
            time.sleep(2.0)
            ss = page.screenshot(full_page=True)
            context.close(); browser.close()
            return ss
    except Exception as e:
        log(f"Wayback æ¸²æŸ“å¼‚å¸¸ ({snapshot_ts}): {e}")
        return None

def try_fetch_public_history():
    tried = []
    for base in DG_LINKS:
        for path in ["/api/history","/history","/api/v1/history","/game/history","/api/records"]:
            url = base.rstrip("/") + path
            tried.append(url)
            try:
                r = requests.get(url, timeout=8)
                if r.status_code == 200:
                    try:
                        return r.json()
                    except:
                        continue
            except:
                continue
    log(f"å°è¯•è¿‡çš„å¯èƒ½å†å²æ¥å£: {tried}")
    return None

def predict_from_history(state):
    hist = state.get("history",[]) or []
    now = now_tz()
    cutoff = now - timedelta(days=HISTORY_LOOKBACK_DAYS)
    recent=[]
    for ev in hist:
        try:
            st = datetime.fromisoformat(ev["start_time"])
            st = st.astimezone(TZ) if st.tzinfo else st.replace(tzinfo=timezone.utc).astimezone(TZ)
            if st >= cutoff:
                recent.append({"kind":ev.get("kind"), "start":st, "duration": ev.get("duration_minutes",0)})
        except Exception:
            continue
    if len(recent) < MIN_HISTORY_EVENTS_FOR_PRED:
        log(f"å†å²äº‹ä»¶ä¸è¶³ (recent={len(recent)}) æ— æ³•é¢„æµ‹")
        return None
    buckets={}
    for ev in recent:
        weekday = ev["start"].weekday(); hour = ev["start"].hour
        bmin = (ev["start"].minute // PRED_BUCKET_MINUTES) * PRED_BUCKET_MINUTES
        key = (ev["kind"], weekday, hour, bmin)
        if key not in buckets: buckets[key] = {"count":0,"durations":[]}
        buckets[key]["count"] += 1
        buckets[key]["durations"].append(ev["duration"])
    candidates=[]
    for k,v in buckets.items():
        if v["count"] >= MIN_HISTORY_EVENTS_FOR_PRED:
            avg_dur = round(sum(v["durations"])/len(v["durations"])) if v["durations"] else 10
            candidates.append({"key":k,"count":v["count"],"avg_duration":avg_dur})
    if not candidates: return None
    candidates.sort(key=lambda x: x["count"], reverse=True)
    best = candidates[0]
    kind, weekday, hour, bmin = best["key"]
    now = now_tz()
    days_ahead = (weekday - now.weekday()) % 7
    predicted_start = (now + timedelta(days=days_ahead)).replace(hour=hour, minute=bmin, second=0, microsecond=0)
    if predicted_start < now - timedelta(minutes=1):
        predicted_start += timedelta(days=7)
    predicted_end = predicted_start + timedelta(minutes=best["avg_duration"])
    return {"kind":kind, "predicted_start":predicted_start, "predicted_end":predicted_end, "avg_duration":best["avg_duration"], "count":best["count"]}

# ---------------- Main & fallback ----------------
def main():
    log("å¼€å§‹ä¸€æ¬¡æ£€æµ‹ run")
    state = load_state()
    screenshot = None
    try:
        screenshot = capture_dg_page()
    except Exception as e:
        log(f"capture_dg_page å¼‚å¸¸: {e}\n{traceback.format_exc()}")
    if screenshot:
        # å®æ—¶è·¯å¾„
        try:
            with open(LAST_SCREENSHOT, "wb") as f: f.write(screenshot)
        except: pass
        img = pil_from_bytes(screenshot); bgr = cv_from_pil(img)
        pts,_,_ = detect_color_points(bgr)
        log(f"å®æ—¶æ£€æµ‹ç‚¹æ•°: {len(pts)}")
        if len(pts) < MIN_POINTS_FOR_REAL:
            log("ç‚¹æ•°ä¸è¶³ï¼Œè¿›å…¥å†å²æ›¿è¡¥æµç¨‹")
            fallback_with_history(state)
            return
        rects = cluster_points_to_boards(pts, bgr.shape)
        boards=[]
        for r in rects:
            boards.append(analyze_board(bgr, r))
        overall, longCount, superCount = classify_overall(boards)
        log(f"å®æ—¶åˆ¤å®š: {overall} (é•¿é¾™/è¶…: {longCount}/{superCount})")
        nowiso = now_tz().isoformat()
        was_active = state.get("active", False)
        is_active_now = overall in ("æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰", "ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰")
        if is_active_now and not was_active:
            state = {"active":True,"kind":overall,"start_time":nowiso,"last_seen":nowiso,"history":state.get("history",[])}
            save_state(state)
            durations = [h.get("duration_minutes",0) for h in state.get("history",[]) if h.get("duration_minutes",0)>0]
            est_min = round(sum(durations)/len(durations)) if durations else 10
            est_end = (now_tz() + timedelta(minutes=est_min)).strftime("%Y-%m-%d %H:%M:%S")
            msg = f"ğŸ”” [DGæé†’ - å³æ™‚] {overall} é–‹å§‹\næ™‚é–“: {nowiso}\né•¿é¾™/è¶…é¾™ æ¡Œæ•¸: {longCount} (è¶…é¾™:{superCount})\nä¼°è¨ˆçµæŸ: {est_end}ï¼ˆç´„ {est_min} åˆ†é˜ï¼ŒåŸºæ–¼æ­·å²ï¼‰"
            send_telegram(msg)
            save_state(state)
        elif is_active_now and was_active:
            state["last_seen"] = nowiso; state["kind"] = overall; save_state(state); log("äº‹ä»¶ä»åœ¨é€²è¡Œï¼Œæ›´æ–° last_seen")
        elif not is_active_now and was_active:
            start_time = datetime.fromisoformat(state.get("start_time"))
            end_time = now_tz()
            duration_minutes = round((end_time - start_time).total_seconds() / 60.0)
            hist = state.get("history",[]); hist.append({"kind":state.get("kind"), "start_time": state.get("start_time"), "end_time": end_time.isoformat(), "duration_minutes": duration_minutes})
            state = {"active":False,"kind":None,"start_time":None,"last_seen":None,"history": hist[-2000:]}
            save_state(state)
            msg = f"âœ… [DGæé†’] {state.get('kind')} å·²çµæŸ\né–‹å§‹: {state.get('start_time')}\nçµæŸ: {end_time.isoformat()}\nå¯¦éš›æŒçºŒ: {duration_minutes} åˆ†é˜"
            send_telegram(msg)
            log("äº‹ä»¶çµæŸï¼Œå·²ç™¼é€é€šçŸ¥ä¸¦è¨˜éŒ„")
        else:
            save_state(state); log("éæ”¾æ°´/ä¸­ä¸Šæ™‚æ®µï¼Œä¸ç™¼é€")
        # ä¿å­˜ summary
        try:
            with open(SUMMARY_FILE,"w",encoding="utf-8") as f:
                json.dump({"ts": now_tz().isoformat(), "overall": overall, "longCount": longCount, "superCount": superCount, "boards": boards[:60]}, f, ensure_ascii=False, indent=2)
        except:
            pass
        return
    else:
        fallback_with_history(state)
        return

def fallback_with_history(state):
    log("è¿›å…¥å†å²æ›¿è¡¥ï¼ˆç«‹å³è§¦å‘ï¼‰")
    # å°è¯• 1) å…¬å…± API  2) Wayback å¿«ç…§
    api_hist = None
    try:
        api_hist = try_fetch_public_history()
    except Exception as e:
        log(f"try_fetch_public_history å¼‚å¸¸: {e}")
    if api_hist:
        norm=[]
        if isinstance(api_hist, list):
            for rec in api_hist:
                st = rec.get("start_time") or rec.get("ts") or rec.get("time")
                end = rec.get("end_time") or rec.get("end")
                dur = rec.get("duration_minutes") or rec.get("duration")
                if st:
                    norm.append({"kind": rec.get("kind","æ”¾æ°´"), "start_time": st, "end_time": end, "duration_minutes": dur or 0})
        elif isinstance(api_hist, dict):
            for key in ("events","history","records"):
                if key in api_hist and isinstance(api_hist[key], list):
                    for rec in api_hist[key]:
                        st = rec.get("start_time") or rec.get("ts") or rec.get("time")
                        end = rec.get("end_time") or rec.get("end")
                        dur = rec.get("duration_minutes") or rec.get("duration")
                        if st:
                            norm.append({"kind": rec.get("kind","æ”¾æ°´"), "start_time": st, "end_time": end, "duration_minutes": dur or 0})
                    break
        if norm:
            hist = state.get("history",[]) or []
            hist.extend(norm)
            state["history"] = hist[-2000:]
            save_state(state)
            log(f"ä»å…¬å…± API å¯¼å…¥ {len(norm)} æ¡å†å²è®°å½•")
    # æ£€æŸ¥å†å²æ˜¯å¦è¶³å¤Ÿ
    hist_recent_count = 0
    for h in state.get("history",[]) or []:
        try:
            st = datetime.fromisoformat(h["start_time"]); st = st.astimezone(TZ) if st.tzinfo else st.replace(tzinfo=timezone.utc).astimezone(TZ)
            if st >= now_tz() - timedelta(days=HISTORY_LOOKBACK_DAYS):
                hist_recent_count += 1
        except:
            continue
    # è‹¥ä»ä¸è¶³ï¼Œä½¿ç”¨ Wayback æŠ“å–å¿«ç…§å¹¶è§£æï¼ˆå°½é‡è·å–â€œå…¨å¸‚åœºâ€å†å²å¿«ç…§ï¼‰
    if hist_recent_count < MIN_HISTORY_EVENTS_FOR_PRED:
        log("å†å²ä¸è¶³ï¼Œå°è¯• Wayback å¿«ç…§æ”¶é›†ï¼ˆè¿™å¯èƒ½æ¯”è¾ƒæ…¢ï¼‰")
        from_date = (now_tz() - timedelta(days=HISTORY_LOOKBACK_DAYS)).strftime("%Y%m%d")
        to_date = now_tz().strftime("%Y%m%d")
        collected = 0
        for base in DG_LINKS:
            timestamps = get_wayback_snapshots(base, from_date=from_date, to_date=to_date, limit=WAYBACK_MAX_SNAPSHOTS)
            if not timestamps:
                log(f"Wayback æœªå‘ç° {base} çš„å¿«ç…§ï¼ˆè¿‡å» {HISTORY_LOOKBACK_DAYS} å¤©ï¼‰")
                continue
            for ts in timestamps[:WAYBACK_MAX_SNAPSHOTS]:
                time.sleep(WAYBACK_RATE_SLEEP)
                ss = fetch_wayback_snapshot_and_screenshot(ts, base)
                if not ss: continue
                try:
                    img = pil_from_bytes(ss); bgr = cv_from_pil(img)
                except Exception:
                    continue
                pts,_,_ = detect_color_points(bgr)
                if len(pts) < MIN_POINTS_FOR_REAL: continue
                rects = cluster_points_to_boards(pts, bgr.shape)
                boards=[]
                for r in rects: boards.append(analyze_board(bgr, r))
                overall, longCount, superCount = classify_overall(boards)
                if overall in ("æ”¾æ°´æ—¶æ®µï¼ˆæé«˜èƒœç‡ï¼‰","ä¸­ç­‰èƒœç‡ï¼ˆä¸­ä¸Šï¼‰"):
                    try:
                        ev_time = datetime.strptime(ts[:14], "%Y%m%d%H%M%S").replace(tzinfo=timezone.utc).astimezone(TZ)
                    except:
                        ev_time = now_tz()
                    rec = {"kind": overall, "start_time": ev_time.isoformat(), "end_time": (ev_time + timedelta(minutes=10)).isoformat(), "duration_minutes": 10, "source":"wayback_snapshot"}
                    hist = state.get("history",[]) or []
                    hist.append(rec)
                    state["history"] = hist[-2000:]
                    save_state(state)
                    collected += 1
                    log(f"Wayback å‘ç°äº‹ä»¶ {overall} @ {ts} -> è®°å½•")
        log(f"Wayback æ”¶é›†ç»“æŸï¼Œæ–°å¢è®°å½• {collected} æ¡")
    else:
        log(f"å†å²å·²è¶³å¤Ÿ (recent {hist_recent_count})ï¼Œè·³è¿‡ Wayback æ”¶é›†")
    # å†æ¬¡è®¡ç®—æœ€è¿‘å†å²æ˜¯å¦è¶³å¤Ÿå¹¶é¢„æµ‹
    hist_now_count = 0
    for h in state.get("history",[]) or []:
        try:
            st = datetime.fromisoformat(h["start_time"]); st = st.astimezone(TZ) if st.tzinfo else st.replace(tzinfo=timezone.utc).astimezone(TZ)
            if st >= now_tz() - timedelta(days=HISTORY_LOOKBACK_DAYS):
                hist_now_count += 1
        except:
            continue
    log(f"å¤„ç†åæœ€è¿‘å†å²æ•°é‡: {hist_now_count}")
    if hist_now_count < MIN_HISTORY_EVENTS_FOR_PRED:
        log("ä»ç„¶æ— æ³•è·å¾—è¶³å¤Ÿå†å²ï¼ˆæœ€è¿‘ 4 å‘¨ï¼‰ï¼Œæš‚æ—¶ä¸å‘é€æ›¿è¡¥æé†’ã€‚ä¼šç»§ç»­æ”¶é›†å¹¶ç­‰å¾…ä¸‹ä¸€æ¬¡è¿è¡Œã€‚")
        save_state(state); return
    pred = predict_from_history(state)
    if not pred:
        log("å†å²ä¸­æœªå‘ç°ç¨³å®šæ¨¡å¼ï¼Œæ— æ›¿è¡¥æé†’ã€‚")
        save_state(state); return
    ps = pred["predicted_start"]; pe = pred["predicted_end"]
    now = now_tz(); lead = timedelta(minutes=PRED_LEAD_MINUTES)
    if (ps - lead) <= now <= pe:
        remaining = max(0, int((pe - now).total_seconds()//60))
        msg = f"ğŸ”” [DGæ›¿è£œé æ¸¬ - æ­·å²å…¨å¸‚å ´] æ ¹æ“šæœ€è¿‘ {HISTORY_LOOKBACK_DAYS} å¤©å…¨å¸‚å ´æ­·å²æ¨¡å¼åµæ¸¬åˆ°å¯èƒ½çš„ã€{pred['kind']}ã€\né æ¸¬é–‹å§‹: {ps.strftime('%Y-%m-%d %H:%M:%S')}\nä¼°è¨ˆçµæŸ: {pe.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆç´„ {pred['avg_duration']} åˆ†é˜ï¼‰\nç›®å‰å‰©é¤˜: ç´„ {remaining} åˆ†é˜\nâ€» æ­¤é€šçŸ¥ç‚ºæ›¿è£œï¼ˆåŸºæ–¼ Wayback / å…¬å…±æ­·å²è³‡æ–™ï¼‰ï¼Œéå³æ™‚å¯¦ç›¤æŠ“å–ã€‚"
        # é¿å…é‡å¤å‘é€ï¼šè‹¥å†å²ä¸­å·²æœ‰ç›¸è¿‘è®°å½•åˆ™è·³è¿‡é‡å¤
        hist = state.get("history",[]) or []
        duplicate=False
        for h in hist[-80:]:
            try:
                if h.get("kind")==pred["kind"]:
                    tm = datetime.fromisoformat(h["start_time"]).astimezone(TZ)
                    if abs((tm - ps).total_seconds()) < 60*5: duplicate=True; break
            except:
                continue
        if not duplicate:
            send_telegram(msg)
            hist.append({"kind":pred["kind"], "start_time": ps.isoformat(), "end_time": pe.isoformat(), "duration_minutes": pred["avg_duration"], "source":"historical_predict"})
            state["history"] = hist[-2000:]
            save_state(state)
            log("å·²å‘é€å†å²æ›¿è¡¥æé†’å¹¶è®°å…¥å†å²")
        else:
            log("å‘ç°è¿‘ä¼¼å†å²äº‹ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤å‘é€")
    else:
        log(f"é¢„æµ‹ä¸‹æ¬¡ {pred['kind']} å¼€å§‹äº {ps.strftime('%Y-%m-%d %H:%M:%S')} (å°šæœªåˆ°æé†’çª—å£)")
    save_state(state)
    return

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"Unhandled exception: {e}\n{traceback.format_exc()}")
        sys.exit(1)
